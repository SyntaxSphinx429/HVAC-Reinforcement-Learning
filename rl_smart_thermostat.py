# -*- coding: utf-8 -*-
"""RL smart thermostat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9RjcYleKcrNDA2LDMGC2KOpls40k7gj
"""

import streamlit as st
import numpy as np
import pandas as pd
import random
import time

# ==========================================
# 1. PAGE CONFIGURATION
# ==========================================
st.set_page_config(page_title="Smart HVAC RL Agent", layout="wide")

st.title("ðŸ¤– Smart HVAC Controller with Q-Learning")
st.markdown("""
This interactive simulation demonstrates a **Reinforcement Learning (RL)** agent controlling an HVAC system.
The agent's goal is to maintain the **Setpoint Temperature** while minimizing **Energy Cost**.
""")

# ==========================================
# 2. CONFIGURATION & HYPERPARAMETERS
# ==========================================
# Q-Learning Parameters
ALPHA = 0.1     # Learning Rate
GAMMA = 0.9     # Discount Factor
EPSILON = 0.1   # Exploration Rate

# Environment Settings
T_IN_RANGE = np.arange(16, 36, 1) # Indoor Temp Range
T_OUT_RANGE = np.arange(24, 39, 2) # Outdoor Temp Range
OCCUPANCY_STATES = [0, 1]          # 0: Empty, 1: Occupied

# Actions: [Off, Low Heat, High Heat, Low Cool, High Cool]
ACTIONS = [0, 1, 2, 3, 4]
ACTION_NAMES = ["OFF", "Low Heat", "High Heat", "Low Cool", "High Cool"]

# Energy Cost for each action (Scalar values)
ACTION_COSTS = {0: 0.0, 1: 0.5, 2: 1.0, 3: 0.6, 4: 1.2}

# Initialize Q-Table (Cached to persist across interaction)
if 'q_table' not in st.session_state:
    num_states = len(T_IN_RANGE) * len(T_OUT_RANGE) * len(OCCUPANCY_STATES)
    num_actions = len(ACTIONS)
    st.session_state.q_table = np.zeros((num_states, num_actions))
    st.session_state.is_trained = False

# ==========================================
# 3. HELPER FUNCTIONS
# ==========================================
def get_state_index(t_in, t_out, occ):
    """Maps real-world values to Q-Table index"""
    idx_in = (np.abs(T_IN_RANGE - t_in)).argmin()
    idx_out = (np.abs(T_OUT_RANGE - t_out)).argmin()
    return (idx_in * len(T_OUT_RANGE) * len(OCCUPANCY_STATES)) + \
           (idx_out * len(OCCUPANCY_STATES)) + occ

def calculate_reward(t_in, action, occupancy, setpoint):
    """Calculates R = -(Cost + ComfortPenalty)"""
    w1 = 1.0 # Weight for Energy
    w2 = 3.0 # Weight for Comfort

    energy_cost = ACTION_COSTS[action]

    if occupancy == 1:
        dist = abs(t_in - setpoint)
        # Give 0 penalty if within small deadband
        comfort_penalty = 0 if dist <= 0.5 else dist
    else:
        comfort_penalty = 0 # Smart Savings logic (Empty room = 0 penalty)

    return -(w1 * energy_cost) - (w2 * comfort_penalty)

def step_environment(t_in, t_out, action):
    """Simulates basic heat transfer physics"""
    # Physics: Thermal Leakage (Room tries to match Outdoor Temp)
    k = 0.1
    leakage = k * (t_out - t_in)

    # Physics: HVAC Impact
    hvac_impact = 0
    if action == 1: hvac_impact = 0.5   # Low Heat
    if action == 2: hvac_impact = 1.5   # High Heat
    if action == 3: hvac_impact = -0.8  # Low Cool
    if action == 4: hvac_impact = -2.0  # High Cool

    new_t_in = t_in + leakage + hvac_impact
    return np.clip(new_t_in, 16, 35)

# ==========================================
# 4. TRAINING SECTION
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ Simulation Settings")
    st.write("Click below to teach the agent!")
    train_btn = st.button("ðŸš€ Train Agent (10k Episodes)")

    if train_btn:
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Training Loop
        q_table = st.session_state.q_table
        target_setpoint = 24.0 # Training Setpoint

        for episode in range(10000):
            current_t_in = random.choice(T_IN_RANGE)
            current_t_out = random.choice(T_OUT_RANGE)
            current_occ = random.choice([0, 1])

            for _ in range(10): # 10 steps per episode
                s_idx = get_state_index(current_t_in, current_t_out, current_occ)

                # Epsilon-Greedy Exploration
                if random.uniform(0, 1) < EPSILON:
                    action = random.choice(ACTIONS)
                else:
                    action = np.argmax(q_table[s_idx])

                next_t_in = step_environment(current_t_in, current_t_out, action)
                reward = calculate_reward(next_t_in, action, current_occ, target_setpoint)
                next_s_idx = get_state_index(next_t_in, current_t_out, current_occ)

                # BELLMAN EQUATION UPDATE
                old_val = q_table[s_idx, action]
                next_max = np.max(q_table[next_s_idx])
                new_val = old_val + ALPHA * (reward + (GAMMA * next_max) - old_val)
                q_table[s_idx, action] = new_val

                current_t_in = next_t_in

            if episode % 1000 == 0:
                progress_bar.progress(episode / 10000)

        st.session_state.is_trained = True
        progress_bar.progress(100)
        status_text.success("Training Complete! Agent is ready.")

# ==========================================
# 5. INTERACTIVE ROOM DEMO
# ==========================================
st.divider()
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("ðŸ  Room Conditions")
    st.info("Adjust the sliders to test the agent.")
    user_setpoint = st.slider("Target Temperature (Â°C)", 18, 30, 24)
    user_outdoor = st.slider("Outdoor Temperature (Â°C)", 24, 40, 33)
    user_indoor = st.number_input("Current Indoor Temp (Â°C)", 16.0, 35.0, 28.0, step=0.5)
    user_occupancy = st.radio("Occupancy Status", ["Empty (0)", "Occupied (1)"], index=1)
    occ_val = 1 if "Occupied" in user_occupancy else 0

with col2:
    st.subheader("ðŸ§  Agent Decision")

    if st.session_state.is_trained:
        # Get Agent's Decision from Q-Table
        state_idx = get_state_index(user_indoor, user_outdoor, occ_val)
        best_action_idx = np.argmax(st.session_state.q_table[state_idx])
        predicted_action = ACTION_NAMES[best_action_idx]

        # Calculate expected reward based on this state
        reward_val = calculate_reward(user_indoor, best_action_idx, occ_val, user_setpoint)

        # Display Results
        st.metric(label="Recommended Action", value=predicted_action)

        # Visual Indicators
        if "Cool" in predicted_action:
            st.info(f"â„ï¸ System is COOLING. Power Usage: {ACTION_COSTS[best_action_idx]}")
        elif "Heat" in predicted_action:
            st.warning(f"ðŸ”¥ System is HEATING. Power Usage: {ACTION_COSTS[best_action_idx]}")
        else:
            st.success("âœ… System is OFF. Saving Energy.")

        st.write(f"**Expected Reward:** {reward_val:.2f} (Target is 0)")

        # Explain Logic
        st.markdown("### Why this action?")
        if occ_val == 0:
            st.caption("The room is **Empty**. The agent chooses OFF to maximize energy savings (Reward = 0).")
        elif abs(user_indoor - user_setpoint) < 1.0:
            st.caption("Temperature is close to Setpoint. Agent minimizes effort.")
        elif user_indoor > user_setpoint and "Cool" in predicted_action:
            st.caption("Room is too hot! Agent is cooling to avoid Comfort Penalty.")
        elif user_indoor < user_setpoint and "Heat" in predicted_action:
             st.caption("Room is too cold! Agent is heating to avoid Comfort Penalty.")

    else:
        st.warning("âš ï¸ Please click 'Train Agent' in the sidebar first!")

# ==========================================
# 6. Q-TABLE VISUALIZATION
# ==========================================
st.divider()
with st.expander("ðŸ“Š View Q-Table (Agent's Brain)"):
    if st.session_state.is_trained:
        # Create readable DataFrame
        df = pd.DataFrame(st.session_state.q_table, columns=ACTION_NAMES)
        st.dataframe(df.head(50))
        st.caption("Showing first 50 states. Rows = States, Columns = Q-Values for each Action.")
    else:
        st.write("Train the agent to see the Q-Table.")